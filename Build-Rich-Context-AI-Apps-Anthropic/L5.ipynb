{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfda5ce-4f4a-4582-b4ae-ab309696eedf",
   "metadata": {},
   "source": [
    "# Lesson 5: Creating an MCP Client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211ebe3-496b-4f54-b811-1b778787ae8f",
   "metadata": {},
   "source": [
    "In the previous lesson, you created an MCP research server that exposes 2 tools. In this lesson, you will make the chatbot communicate to the server through an MCP client. This will make the chatbot MCP compatible. You will continue from where you left off in lesson 4, i.e., you are provided again with the `mcp_project` folder that contains the `research_server.py` file. You'll add to it the MCP chatbot file and update the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc42aa-75b1-44b5-a3d4-d56f77b34cd9",
   "metadata": {},
   "source": [
    "<img src=\"images/lesson_progression.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b159c4-e06e-4a0c-acb4-028e2046ed73",
   "metadata": {},
   "source": [
    "Now you will take the functions `process_query` and `chat_loop` and wrap them in a `MCP_ChatBot` class. To enable the chatbot to communicate to the server, you will add a method that connects to the server through an MCP client, which follows the structure given in this reference code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380fcdc-1703-40a6-9dd0-600c6ba2bdb1",
   "metadata": {},
   "source": [
    "### Adding MCP Client to the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b442185-9c23-47a2-8a7e-f92928a45bed",
   "metadata": {},
   "source": [
    "The MCP_ChatBot class consists of the methods:\n",
    "- `process_query`\n",
    "- `chat_loop`\n",
    "- `connect_to_server_and_run`\n",
    "  \n",
    "and has the following attributes:\n",
    "- session (of type ClientSession)\n",
    "- anthropic: Anthropic                           \n",
    "- available_tools\n",
    "\n",
    "In `connect_to_server_and_run`, the client launches the server and requests the list of tools that the server provides (through the client session). The tool definitions are stored in the variable `available_tools` and are passed in to the LLM in `process_query`.\n",
    "\n",
    "<img src=\"images/tools_discovery.png\" width=\"400\">\n",
    "\n",
    "\n",
    "In `process_query`, when the LLM decides it requires a tool to be executed, the client session sends to the server the tool call request. The returned response is passed in to the LLM. \n",
    "\n",
    "<img src=\"images/tool_invocation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96d918-1854-49b5-8930-f1a6962a3ee9",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c305bb8-6474-420e-82b4-e6daecb65d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4003372221.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    MODEL_NAME = = \"mistralai/Mistral-7B-Instruct-v0.3\"\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %%writefile mcp_project/mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"http://localhost:8989\"\n",
    "# MODEL_NAME = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8080\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "anthropic = OpenAI(\n",
    "    api_key=API_KEY,  # 您的 Anthropic API 密钥\n",
    "    base_url=f\"{INFERENCE_SERVER_URL}/v1\"  # Anthropic 的 API 端点\n",
    ")\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.anthropic = OpenAI(\n",
    "                                api_key=API_KEY,  # 您的 Anthropic API 密钥\n",
    "                                base_url=f\"{INFERENCE_SERVER_URL}/v1\"  # Anthropic 的 API 端点\n",
    "                            )\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        # response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "        #                               model = 'claude-3-7-sonnet-20250219', \n",
    "        #                               tools = self.available_tools, # tools exposed to the LLM\n",
    "        #                               messages = messages)\n",
    "        response = self.anthropic.chat.completions.create(\n",
    "                                    model=MODEL_NAME, # Anthropic 模型名称\n",
    "                                    tools = self.available_tools, # tools exposed to the LLM\n",
    "                                    messages =messages)\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            assistant_content = []\n",
    "            for content in response.content:\n",
    "                if content.type =='text':\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                    if(len(response.content) == 1):\n",
    "                        process_query= False\n",
    "                elif content.type == 'tool_use':\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                    tool_id = content.id\n",
    "                    tool_args = content.input\n",
    "                    tool_name = content.name\n",
    "    \n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Call a tool\n",
    "                    #result = execute_tool(tool_name, tool_args): not anymore needed\n",
    "                    # tool invocation through the client session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append({\"role\": \"user\", \n",
    "                                      \"content\": [\n",
    "                                          {\n",
    "                                              \"type\": \"tool_result\",\n",
    "                                              \"tool_use_id\":tool_id,\n",
    "                                              \"content\": result.content\n",
    "                                          }\n",
    "                                      ]\n",
    "                                    })\n",
    "                    # response = self.anthropic.messages.create(max_tokens = 2024,\n",
    "                    #                   model = 'claude-3-7-sonnet-20250219', \n",
    "                    #                   tools = self.available_tools,\n",
    "                    #                   messages = messages) \n",
    "                   \n",
    "                    response = self.anthropic.chat.completions.create(\n",
    "                            model=MODEL_NAME, # Anthropic 模型名称\n",
    "                            tools = self.available_tools, # tools exposed to the LLM\n",
    "                            messages =messages)\n",
    "                    \n",
    "                    if(len(response.content) == 1 and response.content[0].type == \"text\"):\n",
    "                        print(response.content[0].text)\n",
    "                        process_query= False\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "    \n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                \n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "                \n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "    \n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df7af0-162a-4419-a188-12a2c6058ab2",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca4737-0d67-4429-9379-8228b5c9ebf1",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48b00b-d256-47f0-8002-170ea74876c0",
   "metadata": {},
   "source": [
    "- To open the terminal, run the cell below.\n",
    "- Navigate to the `mcp_project` directory:\n",
    "    - `cd L5/mcp_project`\n",
    "- Activate the virtual environment:\n",
    "    - `source .venv/bin/activate`\n",
    "- Install the additional dependencies:\n",
    "    - `uv add anthropic python-dotenv nest_asyncio`\n",
    "- Run the chatbot:\n",
    "    - `uv run mcp_chatbot.py`\n",
    "- To exit the chatbot, type `quit`.\n",
    "- If you run some queries and would like to access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L5` -> `mcp_project`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

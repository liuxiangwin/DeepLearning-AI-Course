{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2060be6-ec2f-4a5a-b0f4-61dc93139732",
   "metadata": {},
   "source": [
    "# Lesson 3: Adding RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd47f0-692f-4fe4-8f0d-7b8fbe0a894c",
   "metadata": {},
   "source": [
    "**Lesson objective**: Add a document database to a workflow\n",
    "\n",
    "In this lab, youâ€™ll parse a resume and load it into a vector store, and use the agent to run basic queries against the documents. Youâ€™ll use LlamaParse to parse the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaee4d2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to run the notebook cell by cell. Please try to avoid running all cells at once.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1229da0-c307-47ff-a079-dbaaded2ea96",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef42d9b5-fab6-4aa0-9617-4405235f0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ay (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-index-utils-workflow  --quiet\n",
    "!pip install llama-index-llms-openai-like --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install llama-index --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de54d270-81bf-422a-aeb8-f1e83f2153d2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key, get_llama_cloud_api_key\n",
    "from IPython.display import display, HTML\n",
    "from helper import extract_html_content\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e4be1-f8f1-4fcf-90a6-5b70a8567a57",
   "metadata": {},
   "source": [
    "You need nested async for this to work, so let's enable it here. It allows you to nest asyncio event loops within each other. \n",
    "\n",
    "*Note:* In asynchronous programming, the event loop is like a continuous cycle that manages the execution of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ffe277-75f1-475b-9584-9ddf27174abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ['ATHINA_API_KEY'] = \"IhrJrr0krTMRA9ogqi5aaD4ZuYuvMcdG\"\n",
    "\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8989\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "MODEL_NAME = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "model = OpenAILike(\n",
    "  model=MODEL_NAME,\n",
    "  api_key=API_KEY,\n",
    "  api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "  context_window=1234,\n",
    "  is_chat_model=True,  # supports chat completions\n",
    "  is_function_calling_model=True # supports tools/functions in the api\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61267ae9-a841-47f1-9584-437fd1be5816",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bee788-6706-4cb9-84e6-35fa3daac098",
   "metadata": {},
   "source": [
    "You also need two API keys: \n",
    "- OpenAI like you used earlier;\n",
    "- LlamaCloud API key to use LlamaParse to parse the PDFs. In this notebook, you are provided with such a key. For your personal project, you can get a key at cloud.llamaindex.ai for free.\n",
    "\n",
    "LlamaParse is an advanced document parser that can read PDFs, Word files, Powerpoints, Excel spreadsheets, and extract information out of complicated PDFs into a form LLMs find easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb331c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa144829-78b4-409b-a9f7-3b4e71b28e1e",
   "metadata": {},
   "source": [
    "## Performing Retrieval-Augmented Generation (RAG) on a Resume Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19853ba-dc5e-44e2-9395-9c2db18d22c9",
   "metadata": {},
   "source": [
    "### 1. Parsing the Resume Document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e123386-f058-4ab3-a2a2-aeed9ec90fb4",
   "metadata": {},
   "source": [
    "Let's start by parsing a resume.\n",
    "\n",
    "<img width=\"400\" src=\"images/parsing_res.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ef68c-2ffd-41a2-bd50-46fe4bea35ba",
   "metadata": {},
   "source": [
    "Using LLamaParse, you will transform the resume into a list of Document objects. By default, a Document object stores text along with some other attributes:\n",
    "- metadata: a dictionary of annotations that can be appended to the text.\n",
    "- relationships: a dictionary containing relationships to other Documents.\n",
    "  \n",
    "\n",
    "You can tell LlamaParse what kind of document it's parsing, so that it will parse the contents more intelligently. In this case, you tell it that it's reading a resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ccb487-d089-4ac2-844a-4b3bc0f5e223",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff32431-3ae4-446a-a458-c40949ef6f4f",
   "metadata": {
    "height": 164
   },
   "outputs": [],
   "source": [
    "# documents = LlamaParse(\n",
    "#     api_key=llama_cloud_api_key,\n",
    "#     base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "#     result_type=\"markdown\",\n",
    "#     content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "# ).load_data(\n",
    "#     \"data/fake_resume.pdf\",\n",
    "# )\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fdeea-70ee-4422-a488-003a305bcf5a",
   "metadata": {},
   "source": [
    "This gives you a list of Document objects you can feed to a VectorStoreIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1245b406-c0ff-42b8-9498-cb14e2f729bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# print(documents[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df6497-f6b0-4231-8fc2-57a326f2bac6",
   "metadata": {},
   "source": [
    "### 2. Creating a Vector Store Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869382b7-4077-4eb7-81df-a262831c4ebe",
   "metadata": {},
   "source": [
    "\n",
    "<img width=\"400\" src=\"images/vector_store_index.png\">\n",
    "\n",
    "You'll now feed the Document objects to `VectorStoreIndex`. The `VectorStoreIndex` will use an embedding model to embed the text, i.e. turn it into vectors that you can search. You'll be using an embedding model provided by OpenAI, which is why we needed an OpenAI key.\n",
    "\n",
    "The `VectorStoreIndex` will return an index object, which is a data structure that allows you to quickly retrieve relevant context for your query. It's the core foundation for RAG use-cases. You can use indexes to build Query Engines and Chat Engines which enables question & answer and chat over your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d6351c-9e1f-457b-b6d0-d739fb826743",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3551e3b-4212-4705-9b44-4638dc63d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "Settings.llm = model\n",
    "Settings.embed_model = embed_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6816a0-15c1-40f5-8aab-6276002cac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de6653-243b-4ddd-8a1a-efd77dfbb1db",
   "metadata": {},
   "source": [
    "### 3. Creating a Query Engine with the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4460b2-7c72-4944-80f4-642e437d7eff",
   "metadata": {},
   "source": [
    "With an index, you can create a query engine and ask questions. Let's try it out! Asking questions requires an LLM, so let's use OpenAI again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8820d8b0-4749-4414-afbe-cbc3bc6146dd",
   "metadata": {
    "height": 79
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person's name is Sarah Chen. Her most recent job was as a Senior Full Stack Developer at TechFlow Solutions, located in San Francisco, CA.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=model, \n",
    "                                     similarity_top_k=5)\n",
    "response = query_engine.query(\"What is this person's name and what was their most recent job?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443cdeb-fb24-4ad9-abd5-c4bf845ca258",
   "metadata": {},
   "source": [
    "### 4. Storing the Index to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b7836-90cf-4883-a9bc-18fb7a443023",
   "metadata": {},
   "source": [
    "Indexes can be persisted to disk. This is useful in a notebook that you might run several times! In a production setting, you would probably use a hosted vector store of some kind. Let's save your index to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc5fbb0-e700-45d9-9e44-a6deae557a94",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "storage_dir = \"./storage\"\n",
    "\n",
    "index.storage_context.persist(persist_dir=storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e0d88ae-c97d-42d9-a67c-1e3e846a4e11",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ffb6d9-aa9d-4f5b-a743-5c3a0e4dafc9",
   "metadata": {},
   "source": [
    "You can check if your index has already been stored, and if it has, you can reload an index from disk using the `load_index_from_storage` method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733c762c-27d0-4bab-9a72-c5c0e568f9c4",
   "metadata": {
    "height": 147
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# Check if the index is stored on disk\n",
    "if os.path.exists(storage_dir):\n",
    "    # Load the index from disk\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "    restored_index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    print(\"Index not found on disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933e1172-2187-4847-a5a6-fca67eb9f1d6",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information provided, it's not possible to determine the individual's name or their most recent job title. The context includes a job application form and a resume, but neither explicitly provides this information. The application form asks for personal details like first and last name, email, phone, LinkedIn, and project portfolio, but these are not filled in. The resume lists various projects, certifications, languages, interests, and achievements, but it does not include personal details like name or job title.\n"
     ]
    }
   ],
   "source": [
    "response = restored_index.as_query_engine().query(\"What is this person's name and what was their most recent job?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077027c-d4de-4627-9b35-66362039ad54",
   "metadata": {},
   "source": [
    "Congratulations! You have performed retrieval augmented generation (RAG) on a resume document. With proper scaling, this technique can work across databases of thousands of documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709fda1-7cd5-4c9a-b219-ed64d16d9e76",
   "metadata": {},
   "source": [
    "## Making RAG Agentic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d7e4b-5167-4214-a09b-132c62c5370b",
   "metadata": {},
   "source": [
    "With a RAG pipeline in hand, let's turn it into a tool that can be used by an agent to answer questions. This is a stepping-stone towards creating an agentic system that can perform your larger goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30f1ca1b-7e2f-416b-aa42-1adcd7d0fd53",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3ce49-7009-4bb4-a8e7-8f3d543a9ca2",
   "metadata": {},
   "source": [
    "First, create a regular python function that performs a RAG query. It's important to give this function a descriptive name, to mark its input and output types, and to include a docstring (that's the thing in triple quotes) which describes what it does. The framework will give all this metadata to the LLM, which will use it to decide what a tool does and whether to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6837a256-7273-4a63-9f0b-2733a9f143d8",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "def query_resume(q: str) -> str:\n",
    "    \"\"\"Answers questions about a specific resume.\"\"\"\n",
    "    # we're using the query engine we already created above\n",
    "    response = query_engine.query(f\"This is a question about the specific resume we have in our database: {q}\")\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b3802-0452-49f7-a6b2-19d173f18021",
   "metadata": {},
   "source": [
    "The next step is to create the actual tool. There's a utility function, `FunctionTool.from_defaults`, to do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "507c1f77-3f9c-4864-ae8c-2fcbc4f91a30",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "resume_tool = FunctionTool.from_defaults(fn=query_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0b483-dcb0-4b65-b3ba-0311f6e41766",
   "metadata": {},
   "source": [
    "Now you can instantiate a `FunctionCallingAgent` using that tool. There are a number of different agent types supported by LlamaIndex; this one is particularly capable and efficient.\n",
    "\n",
    "You pass it an array of tools (just one in this case), you give it the same LLM we instantiated earlier, and you set Verbose to true so you get a little more info on what your agent is up to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53a2b4bb-f4ef-494a-af61-e12052706945",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/llama_index/core/agent/function_calling/base.py:87: DeprecationWarning: Call to deprecated class FunctionCallingAgent. (FunctionCallingAgent has been rewritten and replaced by newer agents based on llama_index.core.agent.workflow.FunctionAgent.\n",
      "\n",
      "This implementation will be removed in a v0.13.0.\n",
      "\n",
      "See the docs for more information on updated usage: https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
      "  return cls(\n",
      "/opt/app-root/lib64/python3.11/site-packages/deprecated/classic.py:184: DeprecationWarning: Call to deprecated class AgentRunner. (AgentRunner has been deprecated and is not maintained.\n",
      "\n",
      "This implementation will be removed in a v0.13.0.\n",
      "\n",
      "See the docs for more information on updated agent usage: https://docs.llamaindex.ai/en/stable/understanding/agent/)\n",
      "  return old_new1(cls, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tools=[resume_tool],\n",
    "    llm=model,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603f59f-dc5a-47fe-b0d8-0360e49cb9e6",
   "metadata": {},
   "source": [
    "Now you can chat to the agent! Let's ask it a quick question about our applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22722aa1-dcb0-4225-a16e-8ccdf254f288",
   "metadata": {
    "height": 62
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 37a4b45e-a526-4680-b9c8-c19a7826d786. Step input: How many years of experience does the applicant have?\n",
      "Added user message to memory: How many years of experience does the applicant have?\n",
      "=== Calling Function ===\n",
      "Calling function: query_resume with args: {\"q\": \"How many years of experience does the applicant have?\"}\n",
      "=== Function Output ===\n",
      "Based on the provided context, the applicant has over 6 years of professional experience in the field of Full Stack Web Development, with a specialization in React, Node.js, and cloud architecture. Their experience includes leading technical teams, implementing CI/CD pipelines, and architecting microservices-based platforms.\n",
      "> Running step 5f5f20d6-a410-4a99-a7d6-6eca696be617. Step input: None\n",
      "=== LLM Response ===\n",
      "The applicant has over 6 years of professional experience in Full Stack Web Development, specializing in React, Node.js, and cloud architecture. Their experience includes leading technical teams, implementing CI/CD pipelines, and architecting microservices-based platforms.\n",
      "The applicant has over 6 years of professional experience in Full Stack Web Development, specializing in React, Node.js, and cloud architecture. Their experience includes leading technical teams, implementing CI/CD pipelines, and architecting microservices-based platforms.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How many years of experience does the applicant have?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88192e1d-b3a4-4e9a-aa21-35681997c82a",
   "metadata": {},
   "source": [
    "You can see the agent getting the question, adding it to its memory, picking a tool, calling it with appropriate arguments, and getting the output back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e4848-c143-434d-8c00-587435daa29d",
   "metadata": {},
   "source": [
    "## Wrapping the Agentic RAG into a Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50080f70-b004-4a61-b2e3-8a0238f2b972",
   "metadata": {},
   "source": [
    "You've now got a RAG pipeline and an agent. Let's now create a similar agentic RAG from scratch using a workflow, which you'll extend in later lessons. You won't rely on any of the things you've already created.\n",
    "\n",
    "Here's the workflow you will create:\n",
    "<img width=\"400\" src=\"images/rag_workflow.png\">\n",
    "\n",
    "It consists of two steps:\n",
    "1. `set_up` which is triggered by `StartEvent` and emits `QueryEvent`: at this step, the RAG system is set up and the query is passed to the second step;\n",
    "2. `ask_question` which is triggered by `QueryEvent` and emits `StopEvent`: here the response to the query is generated using the RAG query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785f156c-9f06-4ea9-90cc-2f629fc7a7c8",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "659595b3-714f-441a-bd5b-84c36ea76367",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class QueryEvent(Event):\n",
    "    query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f76078-eb50-48d9-af22-47d5504e7e78",
   "metadata": {
    "height": 776
   },
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    storage_dir = \"./storage\"\n",
    "    # llm: OpenAI\n",
    "    llm: model\n",
    "    query_engine: VectorStoreIndex\n",
    "\n",
    "    # the first step will be setup\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "\n",
    "        if not ev.resume_file:\n",
    "            raise ValueError(\"No resume file provided\")\n",
    "\n",
    "        # define an LLM to work with\n",
    "        # self.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        self.llm = model\n",
    "        # ingest the data and set up the query engine\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # you've already ingested your documents\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            # parse and load your documents\n",
    "            # documents = LlamaParse(\n",
    "            #     result_type=\"markdown\",\n",
    "            #     content_guideline_instruction=\"This is a resume, gather related facts together and format it as bullet points with headers\"\n",
    "            # ).load_data(ev.resume_file)\n",
    "\n",
    "            documents = SimpleDirectoryReader(ev.resume_file).load_data()\n",
    "            # embed and index the documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=embed_model\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "\n",
    "        # either way, create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "\n",
    "        # now fire off a query event to trigger the next step\n",
    "        return QueryEvent(query=ev.query)\n",
    "\n",
    "    # the second step will be to ask a question and return a result immediately\n",
    "    @step\n",
    "    async def ask_question(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume we have in our database: {ev.query}\")\n",
    "        return StopEvent(result=response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf8325-7f79-4855-8080-a7df1237929b",
   "metadata": {},
   "source": [
    "You run it like before, giving it a fake resume we created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a06c2823-e2ae-4bc0-990d-e23707df7804",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "The applicant's first place of work is InnovateSoft, located in Oakland, CA. They held the position of Full Stack Developer there from March 2019 to December 2021.\n"
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow(timeout=120, verbose=False)\n",
    "result = await w.run(\n",
    "    resume_file=\"./data/fake_resume.pdf\",\n",
    "    query=\"Where is the first place the applicant worked?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09770f81-5465-4407-b2f3-885dd6a2dc6f",
   "metadata": {},
   "source": [
    "There's nothing in this workflow you haven't done before, it's just making things neat and encapsulated.\n",
    "\n",
    "If you're particularly suspicious, you might notice there's a small bug here: if you run this a second time, with a new resume, this code will find the old resume and not bother to parse it. You don't need to fix that now, but think about how you might fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673e982",
   "metadata": {},
   "source": [
    "## Workflow Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c5f32-a579-444f-8199-618a1a34b696",
   "metadata": {},
   "source": [
    "You can visualize the workflow you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "460b825b-d395-4c7a-ba78-635cddd4ff02",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflows/rag_workflow.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 750px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#FFA07A\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\", \"title\": null}, {\"color\": \"#ADD8E6\", \"id\": \"set_up\", \"label\": \"set_up\", \"shape\": \"box\", \"title\": null}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\", \"title\": null}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"set_up\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"set_up\"}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "isolated": true
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORKFLOW_FILE = \"workflows/rag_workflow.html\"\n",
    "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
    "html_content = extract_html_content(WORKFLOW_FILE)\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7d2ad-998d-4fa9-9c34-588ed333441c",
   "metadata": {},
   "source": [
    "## Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019261f7-f7b0-4729-b79a-56ff097c619f",
   "metadata": {},
   "source": [
    "You've successfully created an agent with RAG tools. In the next lesson, you'll give your agent more complicated tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
